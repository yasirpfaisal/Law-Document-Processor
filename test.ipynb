{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac4676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"USE_TF\"] = \"0\"\n",
    "# This disables TensorFlow usage in transformers and related libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a413e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install docling langchain langchain-docling langchain-huggingface langchain-community sentence-transformers chromadb sqlalchemy psycopg2-binary spacy fastapi uvicorn gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f38ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade \"transformers>=4.40\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adea84bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n",
    "#%pip uninstall numpy -y\n",
    "#%pip install \"numpy<2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6737ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Check GPU availability\n",
    "gpu_available = torch.cuda.is_available()\n",
    "print(f\"GPU Available: {gpu_available}\")\n",
    "\n",
    "# Print PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Print NumPy version\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5c42bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU Name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d13391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "# --- Suppress warnings ---\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7367f2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#detect whether a pdf is text or image\n",
    "\n",
    "from pypdf import PdfReader\n",
    "\n",
    "def pdf_is_image_heavy(path):\n",
    "    reader = PdfReader(path)\n",
    "    text = \"\".join((pg.extract_text()or \"\") for pg in reader.pages)\n",
    "    return len(text.strip())<100 # heuristic: little extractable text -> image-heavy\n",
    "\n",
    "pdf_is_image_heavy(\"my_contract.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e86a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "# Initialize converter\n",
    "converter = DocumentConverter()\n",
    "\n",
    "# Convert a PDF\n",
    "result = converter.convert(\"my_contract.pdf\")\n",
    "\n",
    "# Extract the DoclingDocument (structured representation)\n",
    "doc = result.document  \n",
    "\n",
    "# Show raw structured output\n",
    "print(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1d8726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_docling import DoclingLoader\n",
    "\n",
    "loader = DoclingLoader(\"my_contract.pdf\")\n",
    "docs = loader.load()\n",
    "print(docs[6].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd1ebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#rule based (regex, spacy) extraction of entities\n",
    "def rule_based_extract(text):\n",
    "    results = {}\n",
    "    confidence = {}\n",
    "\n",
    "    # Example: effective date\n",
    "    date_match = re.search(r\"(?:effective\\s+date\\s*[:\\-]?\\s*)(\\w+\\s\\d{1,2},\\s\\d{4})\", text, re.IGNORECASE)\n",
    "    if date_match:\n",
    "        results[\"effective_date\"] = date_match.group(1)\n",
    "        confidence[\"effective_date\"] = \"high\"\n",
    "    else:\n",
    "        confidence[\"effective_date\"] = \"low\"\n",
    "\n",
    "    # Example: party names (look for 'between X and Y')\n",
    "    parties_match = re.search(r\"This agreement.*between\\s+(.+?)\\s+and\\s+(.+?)\\.\", text, re.IGNORECASE)\n",
    "    if parties_match:\n",
    "        results[\"party_names\"] = [parties_match.group(1), parties_match.group(2)]\n",
    "        confidence[\"party_names\"] = \"high\"\n",
    "    else:\n",
    "        confidence[\"party_names\"] = \"low\"\n",
    "\n",
    "    return results, confidence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebe5a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the NER pipeline with a legal-friendly model\n",
    "ner = pipeline(\"ner\", model=\"Jean-Baptiste/roberta-large-ner-english\", aggregation_strategy=\"simple\", framework=\"pt\")\n",
    "def huggingface_fallback(text):\n",
    "    entities = ner(text)\n",
    "    # Post-process entities to fit your output format if needed\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b5d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install langchain-openai\n",
    "\n",
    "#llm fallback extraction of entities\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "import json\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a legal document parser. Extract fields in JSON.\"),\n",
    "    (\"human\", \"Document text:\\n\\n{doc_text}\\n\\nReturn JSON with keys: party_names, effective_date, termination_date, jurisdiction, signatories.\")\n",
    "])\n",
    "\n",
    "def llm_fallback(text):\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"doc_text\": text})\n",
    "    try:\n",
    "        return json.loads(response.content)\n",
    "    except Exception:\n",
    "        return {\"error\": \"Failed to parse JSON\"}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cc760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hybrid controller\n",
    "def hybrid_extraction(text, threshold=\"high\"):\n",
    "    results, confidence = rule_based_extract(text)\n",
    "    final = results.copy()\n",
    "    \n",
    "    # If any key has low confidence, use LLM fallback\n",
    "    if any(c == \"low\" for c in confidence.values()):\n",
    "        llm_results = llm_fallback(text)\n",
    "        results.update(llm_results)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db42ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunking documents\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_docs(docs):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100  #contetxt overlap\n",
    "        \n",
    "    )\n",
    "    return splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dc52ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_docs(docs)\n",
    "\n",
    "print(f\"Original docs: {len(docs)}\")\n",
    "print(f\"Chunked docs: {len(chunks)}\")\n",
    "\n",
    "# Show first chunk\n",
    "print(chunks[0].page_content[:200])\n",
    "print(chunks[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d702a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding model using huggingface transformers + auto moedel avoid sentence transformers dependency issues\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "class HFCustomEmbeddings:\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-mpnet-base-v2\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def embed_text(self, text: str):\n",
    "        import torch\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        return embeddings[0].cpu().numpy()\n",
    "\n",
    "    # âœ… For LangChain compatibility\n",
    "    def embed_documents(self, texts):\n",
    "        return [self.embed_text(t) for t in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.embed_text(text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ce831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "\n",
    "# Suppose chunks is your Docling split output\n",
    "filtered_chunks = filter_complex_metadata(chunks)\n",
    "\n",
    "\n",
    "emb_model = HFCustomEmbeddings(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "        documents=filtered_chunks,\n",
    "        embedding=emb_model,\n",
    "        persist_directory=\"./chroma_db\" #save locally\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51862432",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing siilairyt search with query\n",
    "query = \"What are the deposit amounts?\"\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "for r in results:\n",
    "    print(\"----\")\n",
    "    print(r.page_content[:200])\n",
    "    print(r.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc988a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#addig\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Load FLAN-T5\n",
    "model_name = \"google/flan-t5-large\"  #t5-large/t5-tglobal-base ideal if GPU is available\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# HF pipeline\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "# Wrap in LangChain\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Retriever from your vector store\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Build RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=local_llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\"  # simplest: just concatenate retrieved chunks\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1a419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradio ui for prototyping\n",
    "import gradio as gr\n",
    "\n",
    "qa_chain = None\n",
    "\n",
    "def process_pdf(pdf_file):\n",
    "    global qa_chain\n",
    "\n",
    "    #load\n",
    "    loader = DoclingLoader(pdf_file.name)\n",
    "    docs = loader.load()\n",
    "\n",
    "    #filter metadata\n",
    "    filtered_docs = filter_complex_metadata(docs)\n",
    "\n",
    "    #chunk\n",
    "    chunks = chunk_docs(filtered_docs)  \n",
    "\n",
    "    #embed + vectorstore\n",
    "    emb_model = HFCustomEmbeddings()\n",
    "    vectorstore = Chroma.from_documents(chunks, emb_model)\n",
    "\n",
    "    #QA chain\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm=local_llm, retriever=retriever, chain_type=\"stuff\")\n",
    "    return \"PDF processed. You can now ask questions.\"\n",
    "def answer_query(query):\n",
    "    if qa_chain is None:\n",
    "        return \"Please upload and process a PDF first.\"\n",
    "    return qa_chain.run(query)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daedc325",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## ðŸ“„ Legal Document Q&A\")\n",
    "    with gr.Row():\n",
    "        pdf_input = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n",
    "        upload_btn = gr.Button(\"Process Document\")\n",
    "    status = gr.Textbox(label=\"Status\")\n",
    "    query = gr.Textbox(label=\"Ask a question\")\n",
    "    answer = gr.Textbox(label=\"Answer\")\n",
    "\n",
    "    upload_btn.click(fn=process_pdf, inputs=pdf_input, outputs=status)\n",
    "    query.submit(fn=ask_question, inputs=query, outputs=answer)\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
