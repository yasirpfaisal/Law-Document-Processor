{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aac4676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"USE_TF\"] = \"0\"\n",
    "# This disables TensorFlow usage in transformers and related libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a413e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install docling langchain langchain-docling langchain-huggingface langchain-community sentence-transformers chromadb sqlalchemy psycopg2-binary spacy fastapi uvicorn gradio\n",
    "#pip install --upgrade \"transformers>=4.40\"\n",
    "#pip install pymupdf\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#%pip uninstall numpy -y\n",
    "#%pip install \"numpy<2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6737ab0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n",
      "PyTorch version: 2.7.1+cu118\n",
      "NumPy version: 2.2.6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Check GPU availability\n",
    "gpu_available = torch.cuda.is_available()\n",
    "print(f\"GPU Available: {gpu_available}\")\n",
    "\n",
    "# Print PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Print NumPy version\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65d13391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "# --- Suppress warnings ---\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fae8004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "\n",
    "pdf_path = \"my_contract.pdf\"  \n",
    "\n",
    "\n",
    "def is_image_heavy(pdf_path, text_threshold=200, image_ratio_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Heuristically determine if a PDF is image-heavy.\n",
    "    - text_threshold: minimum total text characters per page to count as text-based\n",
    "    - image_ratio_threshold: fraction of pages with mostly images before we call it image-heavy\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    total_pages = len(doc)\n",
    "    image_pages = 0\n",
    "\n",
    "    for page in doc:\n",
    "        text = page.get_text(\"text\")\n",
    "        images = page.get_images()\n",
    "        # if very little text and images exist, consider this page \"image-heavy\"\n",
    "        if len(text.strip()) < text_threshold and len(images) > 0:\n",
    "            image_pages += 1\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "    ratio = image_pages / total_pages\n",
    "    print(f\"Image-heavy ratio: {ratio:.2f}\")\n",
    "    return ratio >= image_ratio_threshold\n",
    "\n",
    "def load_contract(pdf_path, prefer_docling=True):\n",
    "    \"\"\"\n",
    "    Auto-select loader based on document type.\n",
    "    Returns a list of LangChain Document objects.\n",
    "    \"\"\"\n",
    "    image_heavy = is_image_heavy(pdf_path) \n",
    "    if image_heavy:\n",
    "        print(\"Detected image-heavy PDF, using DoclingLoader.\")\n",
    "        loader = DoclingLoader(pdf_path)\n",
    "    else:\n",
    "        print(\"Detected text-heavy PDF, using PyMuPDFLoader.\")\n",
    "        loader = PyMuPDFLoader(pdf_path)\n",
    "        \n",
    "    return loader\n",
    "\n",
    "    # Step 3: Load and clean documents\n",
    "    docs = loader.load()\n",
    "    docs = filter_complex_metadata(docs)\n",
    "    return docs\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa81acdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json \n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import string as str\n",
    "INDEX_PATH = Path(\"./index.json\")\n",
    "metadata = []\n",
    "def file_sha256(file_path):\n",
    "    \"\"\"Compute SHA256 hash of a file.\"\"\"\n",
    "    h = hashlib.sha256()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def load_index():\n",
    "    \"\"\"Load index metadata from JSON file.\"\"\"\n",
    "    if INDEX_PATH.exists():\n",
    "        return json.loads(INDEX_PATH.read_text())\n",
    "    return {\"documents\": {}}\n",
    "\n",
    "def save_index(index: dict):\n",
    "    \"\"\"Save index metadata to JSON file.using atomic save to avoid corruption.\"\"\"\n",
    "    tmp_path = INDEX_PATH.with_suffix(\".tmp\")\n",
    "    tmp_path.write_text(json.dumps(index, indent=2), encoding=\"utf-8\")\n",
    "    tmp_path.replace(INDEX_PATH)  # atomic replace\n",
    "\n",
    "\n",
    "\n",
    "def register_doc(doc_hash, pdf_path, persist_dir, extra=metadata):\n",
    "    \"\"\"Register a document hash with its metadata in the index.\"\"\"\n",
    "    idx = load_index()\n",
    "    idx[\"documents\"][doc_hash] = {\n",
    "        \"filename\": filename,\n",
    "        \"persist_dir\": persist_dir,\n",
    "        \"registered_at\": datetime.utcnow().isoformat(),\n",
    "        \"extra\": extra or {}\n",
    "    }\n",
    "    save_index(idx)\n",
    "    \n",
    "def is_doc_registered(doc_hash: str):\n",
    "    \"\"\"Check if a document hash is already registered in the index.\"\"\"\n",
    "    idx = load_index()\n",
    "    return doc_hash in idx[\"documents\"]\n",
    "\n",
    "def get_doc_entry(doc_hash: str):\n",
    "    \"\"\"Get the metadata entry for a document hash.\"\"\"\n",
    "    idx = load_index()\n",
    "    return idx[\"documents\"].get(doc_hash, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bd1ebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#rule based (regex, spacy) extraction of entities\n",
    "def rule_based_extract(text):\n",
    "    results = {}\n",
    "    confidence = {}\n",
    "\n",
    "    # Example: effective date\n",
    "    date_match = re.search(r\"(?:effective\\s+date\\s*[:\\-]?\\s*)(\\w+\\s\\d{1,2},\\s\\d{4})\", text, re.IGNORECASE)\n",
    "    if date_match:\n",
    "        results[\"effective_date\"] = date_match.group(1)\n",
    "        confidence[\"effective_date\"] = \"high\"\n",
    "    else:\n",
    "        confidence[\"effective_date\"] = \"low\"\n",
    "\n",
    "    # Example: party names (look for 'between X and Y')\n",
    "    parties_match = re.search(r\"This agreement.*between\\s+(.+?)\\s+and\\s+(.+?)\\.\", text, re.IGNORECASE)\n",
    "    if parties_match:\n",
    "        results[\"party_names\"] = [parties_match.group(1), parties_match.group(2)]\n",
    "        confidence[\"party_names\"] = \"high\"\n",
    "    else:\n",
    "        confidence[\"party_names\"] = \"low\"\n",
    "\n",
    "    return results, confidence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eebe5a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the NER pipeline with a legal-friendly model\n",
    "ner = pipeline(\"ner\", model=\"Jean-Baptiste/roberta-large-ner-english\", aggregation_strategy=\"simple\", framework=\"pt\")\n",
    "def huggingface_fallback(text):\n",
    "    entities = ner(text)\n",
    "    # Post-process entities to fit your output format if needed\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8202d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_contract(\"my_contract.pdf\")\n",
    "print(len(docs))\n",
    "print(docs[0].page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67b5d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install langchain-openai\n",
    "\n",
    "#llm fallback extraction of entities\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "import json\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a legal document parser. Extract fields in JSON.\"),\n",
    "    (\"human\", \"Document text:\\n\\n{doc_text}\\n\\nReturn JSON with keys: party_names, effective_date, termination_date, jurisdiction, signatories.\")\n",
    "])\n",
    "\n",
    "def llm_fallback(text):\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"doc_text\": text})\n",
    "    try:\n",
    "        return json.loads(response.content)\n",
    "    except Exception:\n",
    "        return {\"error\": \"Failed to parse JSON\"}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0cc760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hybrid controller\n",
    "def hybrid_extraction(text, threshold=\"high\"):\n",
    "    results, confidence = rule_based_extract(text)\n",
    "    final = results.copy()\n",
    "    \n",
    "    # If any key has low confidence, use LLM fallback\n",
    "    if any(c == \"low\" for c in confidence.values()):\n",
    "        llm_results = llm_fallback(text)\n",
    "        results.update(llm_results)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0db42ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunking documents\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_docs(docs):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100  #contetxt overlap\n",
    "        \n",
    "    )\n",
    "    return splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dc52ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_docs(docs)\n",
    "\n",
    "print(f\"Original docs: {len(docs)}\")\n",
    "print(f\"Chunked docs: {len(chunks)}\")\n",
    "\n",
    "# Show first chunk\n",
    "print(chunks[0].page_content[:200])\n",
    "print(chunks[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31d702a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding model using huggingface transformers + auto moedel avoid sentence transformers dependency issues\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "class HFCustomEmbeddings:\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-mpnet-base-v2\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def embed_text(self, text: str):\n",
    "        import torch\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        return embeddings[0].cpu().numpy()\n",
    "\n",
    "    # âœ… For LangChain compatibility\n",
    "    def embed_documents(self, texts):\n",
    "        return [self.embed_text(t) for t in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.embed_text(text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85ce831d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvectorstores\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m filter_complex_metadata\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m chunks = chunk_docs(\u001b[43mdocs\u001b[49m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Suppose chunks is your Docling split output\u001b[39;00m\n\u001b[32m      4\u001b[39m filtered_chunks = filter_complex_metadata(chunks)\n",
      "\u001b[31mNameError\u001b[39m: name 'docs' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "chunks = chunk_docs(docs)\n",
    "# Suppose chunks is your Docling split output\n",
    "filtered_chunks = filter_complex_metadata(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51862432",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorstore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#testing similarity search with query\u001b[39;00m\n\u001b[32m      2\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mWhat are the deposit amounts?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m results = \u001b[43mvectorstore\u001b[49m.similarity_search(query, k=\u001b[32m3\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m----\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'vectorstore' is not defined"
     ]
    }
   ],
   "source": [
    "#testing similarity search with query\n",
    "query = \"What are the deposit amounts?\"\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "for r in results:\n",
    "    print(\"----\")\n",
    "    print(r.page_content[:200])\n",
    "    print(r.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2227aa93",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filtered_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[32m      3\u001b[39m emb_model = HFCustomEmbeddings(\u001b[33m\"\u001b[39m\u001b[33msentence-transformers/all-mpnet-base-v2\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m vectorstore = Chroma.from_documents(\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m         documents=\u001b[43mfiltered_chunks\u001b[49m,\n\u001b[32m      7\u001b[39m         embedding=emb_model,\n\u001b[32m      8\u001b[39m         persist_directory=\u001b[33m\"\u001b[39m\u001b[33m./chroma_db\u001b[39m\u001b[33m\"\u001b[39m )\u001b[38;5;66;03m#save locally\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'filtered_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "emb_model = HFCustomEmbeddings(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "        documents=filtered_chunks,\n",
    "        embedding=emb_model,\n",
    "        persist_directory=\"./chroma_db\" )#save locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc988a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vectorstore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m local_llm = HuggingFacePipeline(pipeline=pipe)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Retriever from your vector store\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m retriever = \u001b[43mvectorstore\u001b[49m.as_retriever(search_kwargs={\u001b[33m\"\u001b[39m\u001b[33mk\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m3\u001b[39m})\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Build RetrievalQA chain\u001b[39;00m\n\u001b[32m     27\u001b[39m qa_chain = RetrievalQA.from_chain_type(\n\u001b[32m     28\u001b[39m     llm=local_llm,\n\u001b[32m     29\u001b[39m     retriever=retriever,\n\u001b[32m     30\u001b[39m     chain_type=\u001b[33m\"\u001b[39m\u001b[33mstuff\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# simplest: just concatenate retrieved chunks\u001b[39;00m\n\u001b[32m     31\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'vectorstore' is not defined"
     ]
    }
   ],
   "source": [
    "#addig\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Load FLAN-T5\n",
    "model_name = \"google/flan-t5-large\"  #t5-large/t5-tglobal-base ideal if GPU is available\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# HF pipeline\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "# Wrap in LangChain\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Retriever from your vector store\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Build RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=local_llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\"  # simplest: just concatenate retrieved chunks\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa1a419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADIO UI (REVISED - cell [39])\n",
    "\n",
    "import gradio as gr\n",
    "from pathlib import Path\n",
    "\n",
    "# Make sure qa_chain is accessible\n",
    "qa_chain = None\n",
    "\n",
    "def process_pdf(pdf_path):\n",
    "    global qa_chain\n",
    "\n",
    "    # Fix 1: Handle no file input\n",
    "    if pdf_path is None:\n",
    "        return \"Error: Please upload a PDF file first.\"\n",
    "\n",
    "    doc_hash = file_sha256(pdf_path)\n",
    "    persist_dir = f\"./chroma_db/{doc_hash}\"\n",
    "    filename = Path(pdf_path).name # Define filename here\n",
    "\n",
    "    # This is the embedding model you initialized earlier\n",
    "    # You may want to define it globally if it's not already\n",
    "    emb_model = HFCustomEmbeddings(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "    if is_doc_registered(doc_hash):\n",
    "        entry = get_doc_entry(doc_hash)\n",
    "        # Load the existing vectorstore\n",
    "        vectorstore = Chroma(persist_directory=entry[\"persist_dir\"], embedding_function=emb_model)\n",
    "        \n",
    "        # Fix 4: Re-create the QA chain for the cached document\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "        qa_chain = RetrievalQA.from_chain_type(llm=local_llm, retriever=retriever, chain_type=\"stuff\")\n",
    "        \n",
    "        return f\"Document '{filename}' is already processed. You can now ask questions.\"\n",
    "\n",
    "    # --- Processing for a new document ---\n",
    "    \n",
    "    # load_contract now correctly returns a loader object\n",
    "    loader = load_contract(pdf_path) \n",
    "    docs = loader.load()\n",
    "\n",
    "    # Filter metadata\n",
    "    filtered_docs = filter_complex_metadata(docs)\n",
    "\n",
    "    # Chunk\n",
    "    chunks = chunk_docs(filtered_docs)\n",
    "    \n",
    "    # Embed and create vectorstore\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=chunks, \n",
    "        embedding=emb_model,\n",
    "        persist_directory=persist_dir # Persist while creating\n",
    "    )\n",
    "\n",
    "    # Create QA chain\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm=local_llm, retriever=retriever, chain_type=\"stuff\")\n",
    "\n",
    "    # Register the document after successful processing\n",
    "    metadata = {\n",
    "        \"original_path\": filename,\n",
    "        \"persist_dir\": persist_dir,\n",
    "        \"num_chunks\": len(chunks)\n",
    "    }\n",
    "    # Pass necessary variables to register_doc\n",
    "    register_doc(doc_hash, pdf_path, persist_dir, filename, extra=metadata) # You need to adjust register_doc to accept filename\n",
    "\n",
    "    return f\"PDF '{filename}' processed. You can now ask questions.\"\n",
    "\n",
    "def answer_query(query):\n",
    "    if qa_chain is None:\n",
    "        return \"Please upload and process a PDF first.\"\n",
    "    \n",
    "    # Fix 5: Use .invoke() instead of the deprecated .run()\n",
    "    # The output of RetrievalQA is a dictionary, we need the 'result' key.\n",
    "    result = qa_chain.invoke({\"query\": query})\n",
    "    return result['result']\n",
    "\n",
    "# You will also need to slightly modify register_doc in cell [18]\n",
    "# to accept 'filename' as a parameter\n",
    "def register_doc(doc_hash, pdf_path, persist_dir, filename, extra=metadata): # Added filename\n",
    "    \"\"\"Register a document hash with its metadata in the index.\"\"\"\n",
    "    idx = load_index()\n",
    "    idx[\"documents\"][doc_hash] = {\n",
    "        \"filename\": filename, # Use the passed filename\n",
    "        \"persist_dir\": persist_dir,\n",
    "        \"registered_at\": datetime.utcnow().isoformat(),\n",
    "        \"extra\": extra or {}\n",
    "    }\n",
    "    save_index(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "daedc325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image-heavy ratio: 0.00\n",
      "Detected text-heavy PDF, using PyMuPDFLoader.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (836 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## ðŸ“„ Legal Document Q&A\")\n",
    "    with gr.Row():\n",
    "        pdf_input = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n",
    "        upload_btn = gr.Button(\"Process Document\")\n",
    "    status = gr.Textbox(label=\"Status\")\n",
    "    query = gr.Textbox(label=\"Ask a question\")\n",
    "    answer = gr.Textbox(label=\"Answer\")\n",
    "\n",
    "    upload_btn.click(fn=process_pdf, inputs=pdf_input, outputs=status)\n",
    "    query.submit(fn=answer_query, inputs=query, outputs=answer)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3967371e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
