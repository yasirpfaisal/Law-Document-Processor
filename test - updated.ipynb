{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aac4676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"USE_TF\"] = \"0\"\n",
    "# This disables TensorFlow usage in transformers and related libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a413e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install docling langchain langchain-docling langchain-huggingface langchain-community sentence-transformers chromadb sqlalchemy psycopg2-binary spacy fastapi uvicorn gradio\n",
    "#pip install --upgrade \"transformers>=4.40\"\n",
    "#pip install pymupdf\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#%pip uninstall numpy -y\n",
    "#%pip install \"numpy<2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6737ab0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n",
      "PyTorch version: 2.7.1+cu118\n",
      "NumPy version: 2.2.6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Check GPU availability\n",
    "gpu_available = torch.cuda.is_available()\n",
    "print(f\"GPU Available: {gpu_available}\")\n",
    "\n",
    "# Print PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Print NumPy version\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d13391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "# --- Suppress warnings ---\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fae8004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "\n",
    "pdf_path = \"my_contract.pdf\"  \n",
    "\n",
    "\n",
    "def is_image_heavy(pdf_path, text_threshold=200, image_ratio_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Heuristically determine if a PDF is image-heavy.\n",
    "    - text_threshold: minimum total text characters per page to count as text-based\n",
    "    - image_ratio_threshold: fraction of pages with mostly images before we call it image-heavy\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    total_pages = len(doc)\n",
    "    image_pages = 0\n",
    "\n",
    "    for page in doc:\n",
    "        text = page.get_text(\"text\")\n",
    "        images = page.get_images()\n",
    "        # if very little text and images exist, consider this page \"image-heavy\"\n",
    "        if len(text.strip()) < text_threshold and len(images) > 0:\n",
    "            image_pages += 1\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "    ratio = image_pages / total_pages\n",
    "    print(f\"Image-heavy ratio: {ratio:.2f}\")\n",
    "    return ratio >= image_ratio_threshold\n",
    "\n",
    "def load_contract(pdf_path, prefer_docling=True):\n",
    "    \"\"\"\n",
    "    Auto-select loader based on document type.\n",
    "    Returns a list of LangChain Document objects.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if is_image_heavy(pdf_path) and prefer_docling:\n",
    "            print(\"detected image heavy pdf, use docling loader\")   \n",
    "            from langchain_community.document_loaders import DoclingLoader\n",
    "            loader = DoclingLoader(pdf_path)\n",
    "        else:\n",
    "            print(\"detected text heacy pdf, use PyMuPDFLoader\")\n",
    "            loader = PyMuPDFLoader(pdf_path)\n",
    "\n",
    "        docs = loader.load()\n",
    "\n",
    "        docs = filter_complex_metadata(docs)\n",
    "        return docs\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Fallback to PyPDFLoader due to error: {e}\")\n",
    "        fallback_loader = PyPDFLoader(pdf_path)\n",
    "        return fallback_loader.load()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5bd1ebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#rule based (regex, spacy) extraction of entities\n",
    "def rule_based_extract(text):\n",
    "    results = {}\n",
    "    confidence = {}\n",
    "\n",
    "    # Example: effective date\n",
    "    date_match = re.search(r\"(?:effective\\s+date\\s*[:\\-]?\\s*)(\\w+\\s\\d{1,2},\\s\\d{4})\", text, re.IGNORECASE)\n",
    "    if date_match:\n",
    "        results[\"effective_date\"] = date_match.group(1)\n",
    "        confidence[\"effective_date\"] = \"high\"\n",
    "    else:\n",
    "        confidence[\"effective_date\"] = \"low\"\n",
    "\n",
    "    # Example: party names (look for 'between X and Y')\n",
    "    parties_match = re.search(r\"This agreement.*between\\s+(.+?)\\s+and\\s+(.+?)\\.\", text, re.IGNORECASE)\n",
    "    if parties_match:\n",
    "        results[\"party_names\"] = [parties_match.group(1), parties_match.group(2)]\n",
    "        confidence[\"party_names\"] = \"high\"\n",
    "    else:\n",
    "        confidence[\"party_names\"] = \"low\"\n",
    "\n",
    "    return results, confidence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eebe5a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the NER pipeline with a legal-friendly model\n",
    "ner = pipeline(\"ner\", model=\"Jean-Baptiste/roberta-large-ner-english\", aggregation_strategy=\"simple\", framework=\"pt\")\n",
    "def huggingface_fallback(text):\n",
    "    entities = ner(text)\n",
    "    # Post-process entities to fit your output format if needed\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8202d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image-heavy ratio: 0.00\n",
      "detected text heacy pdf, use PyMuPDFLoader\n",
      "16\n",
      "Landlord : LIM JIT WENG \n",
      "Tenant : FAWAZ PARVEZ WADEKAR \n",
      "             AKASH KONNA \n",
      "             YASIR PULIKKAL \n",
      " \n",
      " \n",
      "DATED THIS  1st    DAY OF    FEBRUARY   2023 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "BETWEEN \n",
      " \n",
      " \n",
      " \n",
      "LIM JIT WENG \n",
      "(LANDLORD) \n",
      " \n",
      " \n",
      " \n",
      "AND \n",
      " \n",
      " \n",
      " \n",
      "FAWAZ PARVEZ WADEKAR \n",
      "AKASH KONNA \n",
      "YASIR PULIKKAL \n",
      "(TENANT) \n",
      " \n",
      " \n",
      "******\n"
     ]
    }
   ],
   "source": [
    "docs = load_contract(\"my_contract.pdf\")\n",
    "print(len(docs))\n",
    "print(docs[0].page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67b5d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install langchain-openai\n",
    "\n",
    "#llm fallback extraction of entities\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "import json\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a legal document parser. Extract fields in JSON.\"),\n",
    "    (\"human\", \"Document text:\\n\\n{doc_text}\\n\\nReturn JSON with keys: party_names, effective_date, termination_date, jurisdiction, signatories.\")\n",
    "])\n",
    "\n",
    "def llm_fallback(text):\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"doc_text\": text})\n",
    "    try:\n",
    "        return json.loads(response.content)\n",
    "    except Exception:\n",
    "        return {\"error\": \"Failed to parse JSON\"}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0cc760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hybrid controller\n",
    "def hybrid_extraction(text, threshold=\"high\"):\n",
    "    results, confidence = rule_based_extract(text)\n",
    "    final = results.copy()\n",
    "    \n",
    "    # If any key has low confidence, use LLM fallback\n",
    "    if any(c == \"low\" for c in confidence.values()):\n",
    "        llm_results = llm_fallback(text)\n",
    "        results.update(llm_results)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db42ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunking documents\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_docs(docs):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100  #contetxt overlap\n",
    "        \n",
    "    )\n",
    "    return splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dc52ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_docs(docs)\n",
    "\n",
    "print(f\"Original docs: {len(docs)}\")\n",
    "print(f\"Chunked docs: {len(chunks)}\")\n",
    "\n",
    "# Show first chunk\n",
    "print(chunks[0].page_content[:200])\n",
    "print(chunks[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d702a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding model using huggingface transformers + auto moedel avoid sentence transformers dependency issues\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "class HFCustomEmbeddings:\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-mpnet-base-v2\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def embed_text(self, text: str):\n",
    "        import torch\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        return embeddings[0].cpu().numpy()\n",
    "\n",
    "    # âœ… For LangChain compatibility\n",
    "    def embed_documents(self, texts):\n",
    "        return [self.embed_text(t) for t in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.embed_text(text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ce831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "\n",
    "# Suppose chunks is your Docling split output\n",
    "filtered_chunks = filter_complex_metadata(chunks)\n",
    "\n",
    "\n",
    "emb_model = HFCustomEmbeddings(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "        documents=filtered_chunks,\n",
    "        embedding=emb_model,\n",
    "        persist_directory=\"./chroma_db\" #save locally\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51862432",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing siilairyt search with query\n",
    "query = \"What are the deposit amounts?\"\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "for r in results:\n",
    "    print(\"----\")\n",
    "    print(r.page_content[:200])\n",
    "    print(r.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc988a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#addig\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Load FLAN-T5\n",
    "model_name = \"google/flan-t5-large\"  #t5-large/t5-tglobal-base ideal if GPU is available\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# HF pipeline\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "# Wrap in LangChain\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Retriever from your vector store\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Build RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=local_llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\"  # simplest: just concatenate retrieved chunks\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1a419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradio ui for prototyping\n",
    "import gradio as gr\n",
    "\n",
    "qa_chain = None\n",
    "\n",
    "def process_pdf(pdf_file):\n",
    "    global qa_chain\n",
    "\n",
    "    #load\n",
    "    loader = load_contract(pdf_file.name)\n",
    "    docs = loader.load()\n",
    "\n",
    "    #filter metadata\n",
    "    filtered_docs = filter_complex_metadata(docs)\n",
    "\n",
    "    #chunk\n",
    "    chunks = chunk_docs(filtered_docs)  \n",
    "\n",
    "    #embed + vectorstore\n",
    "    emb_model = HFCustomEmbeddings()\n",
    "    vectorstore = Chroma.from_documents(chunks, emb_model)\n",
    "\n",
    "    #QA chain\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm=local_llm, retriever=retriever, chain_type=\"stuff\")\n",
    "    return \"PDF processed. You can now ask questions.\"\n",
    "def answer_query(query):\n",
    "    if qa_chain is None:\n",
    "        return \"Please upload and process a PDF first.\"\n",
    "    return qa_chain.run(query)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daedc325",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## ðŸ“„ Legal Document Q&A\")\n",
    "    with gr.Row():\n",
    "        pdf_input = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n",
    "        upload_btn = gr.Button(\"Process Document\")\n",
    "    status = gr.Textbox(label=\"Status\")\n",
    "    query = gr.Textbox(label=\"Ask a question\")\n",
    "    answer = gr.Textbox(label=\"Answer\")\n",
    "\n",
    "    upload_btn.click(fn=process_pdf, inputs=pdf_input, outputs=status)\n",
    "    query.submit(fn=ask_question, inputs=query, outputs=answer)\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
